// Copyright 2018 MaidSafe.net limited.
//
// This SAFE Network Software is licensed to you under The General Public License (GPL), version 3.
// Unless required by applicable law or agreed to in writing, the SAFE Network Software distributed
// under the GPL Licence is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied. Please review the Licences for the specific language governing
// permissions and limitations relating to use of the SAFE Network Software.

use super::{
    small_encryptor::SmallEncryptor, utils, SelfEncryptionError, Storage, MAX_CHUNK_SIZE,
    MIN_CHUNK_SIZE,
};
use crate::data_map::{ChunkDetails, DataMap};
use std::convert::From;
pub const MIN: u64 = 3 * MIN_CHUNK_SIZE as u64;
pub const MAX: u64 = 3 * MAX_CHUNK_SIZE as u64;

// An encryptor for data which will be split into exactly three chunks (i.e. size is between
// `3 * MIN_CHUNK_SIZE` and `3 * MAX_CHUNK_SIZE` inclusive).  Only `close()` will actually cause
// chunks to be stored.  Until then, data is held internally in `buffer`.
pub struct MediumEncryptor<S: Storage + Send + Sync> {
    pub storage: S,
    pub buffer: Vec<u8>,
    original_chunks: Option<Vec<ChunkDetails>>,
}

impl<S> MediumEncryptor<S>
where
    S: Storage + 'static + Send + Sync,
{
    // Constructor for use with pre-existing `DataMap::Chunks` where there are exactly three chunks.
    // Retrieves the chunks from storage and decrypts them to its internal `buffer`.
    #[allow(clippy::new_ret_no_self)]
    pub async fn new(
        storage: S,
        chunks: Vec<ChunkDetails>,
    ) -> Result<MediumEncryptor<S>, SelfEncryptionError<S::Error>> {
        debug_assert_eq!(chunks.len(), 3);
        debug_assert!(MIN <= chunks.iter().fold(0, |acc, chunk| acc + chunk.source_size));
        debug_assert!(chunks.iter().fold(0, |acc, chunk| acc + chunk.source_size) <= MAX);

        let mut data = Vec::with_capacity(chunks.len());
        for (index, chunk) in chunks.iter().enumerate() {
            let pad_key_iv = utils::get_pad_key_and_iv(index, &chunks);
            let chunk = storage.get(&chunk.hash).await?;
            let decrypted_chunk = utils::decrypt_chunk(&chunk, pad_key_iv)?;

            data.push(decrypted_chunk);
        }

        let init = Vec::with_capacity(MAX as usize);
        let buffer = data.into_iter().fold(init, |mut buffer, data| {
            buffer.extend(data);
            buffer
        });

        Ok(MediumEncryptor {
            storage,
            buffer,
            original_chunks: Some(chunks),
        })
    }

    // Simply appends to internal buffer assuming the size limit is not exceeded.  No chunks are
    // generated by this call.
    pub async fn write(mut self, data: &[u8]) -> Result<Self, SelfEncryptionError<S::Error>> {
        debug_assert!(data.len() as u64 + self.len() <= MAX);
        self.original_chunks = None;
        self.buffer.extend_from_slice(data);
        Ok(self)
    }

    // This finalises the encryptor - it should not be used again after this call.  Exactly three
    // chunks will be generated and stored by calling this unless the encryptor didn't receive any
    // `write()` calls.
    pub async fn close(mut self) -> Result<(DataMap, S), SelfEncryptionError<S::Error>> {
        if let Some(chunks) = self.original_chunks {
            return Ok((DataMap::Chunks(chunks), self.storage));
        }

        let mut all_chunks;
        let mut chunk_details;

        {
            // Third the contents, with the extra single or two bytes in the last chunk.
            let chunk_contents = vec![
                &self.buffer[..(self.buffer.len() / 3)],
                &self.buffer[(self.buffer.len() / 3)..(2 * (self.buffer.len() / 3))],
                &self.buffer[(2 * (self.buffer.len() / 3))..],
            ];
            // Note the pre-encryption hashes and sizes.
            chunk_details = vec![];
            for (index, contents) in chunk_contents.iter().enumerate() {
                chunk_details.push(ChunkDetails {
                    chunk_num: index as u32,
                    hash: vec![],
                    pre_hash: self.storage.generate_address(contents),
                    source_size: contents.len() as u64,
                });
            }
            // Encrypt the chunks and note the post-encryption hashes
            let partial_details = chunk_details.clone();
            all_chunks = Vec::with_capacity(chunk_contents.len());
            // FIXME: rust-nightly requires this to be mutable while rust-stable does not
            #[allow(unused)]
            for (index, (contents, mut details)) in chunk_contents
                .iter()
                .zip(chunk_details.iter_mut())
                .enumerate()
            {
                let pad_key_iv = utils::get_pad_key_and_iv(index, &partial_details);
                let encrypted_contents = utils::encrypt_chunk(contents, pad_key_iv)?;

                let hash = self.storage.generate_address(&encrypted_contents);
                details.hash = hash.to_vec();
                all_chunks.push(self.storage.put(hash.to_vec(), encrypted_contents).await?);
            }
        }

        Ok((DataMap::Chunks(chunk_details), self.storage))
    }

    pub fn len(&self) -> u64 {
        self.buffer.len() as u64
    }

    pub fn is_empty(&self) -> bool {
        self.buffer.is_empty()
    }
}

impl<S: Storage + Send + Sync> From<SmallEncryptor<S>> for MediumEncryptor<S> {
    fn from(small_encryptor: SmallEncryptor<S>) -> MediumEncryptor<S> {
        MediumEncryptor {
            storage: small_encryptor.storage,
            buffer: small_encryptor.buffer,
            original_chunks: None,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::{
        super::{
            small_encryptor::{self, SmallEncryptor},
            utils, MAX_CHUNK_SIZE,
        },
        *,
    };
    use crate::{
        data_map::DataMap,
        self_encryptor::SelfEncryptor,
        test_helpers::{new_test_rng, random_bytes, Blob, SimpleStorage},
    };
    use rand::Rng;
    use unwrap::unwrap;

    #[test]
    fn consts() {
        assert_eq!(MIN, small_encryptor::MAX + 1);
    }

    // Writes all of `data` to a new encryptor in a single call, then closes and reads back via
    // a `SelfEncryptor`.
    async fn basic_write_and_close(data: &[u8]) {
        let (data_map, storage) = {
            let storage = SimpleStorage::new();
            let mut encryptor = unwrap!(SmallEncryptor::new(storage, vec![])
                .await
                .map(MediumEncryptor::from));
            assert_eq!(encryptor.len(), 0);
            assert!(encryptor.is_empty());
            encryptor = unwrap!(encryptor.write(data).await);
            assert_eq!(encryptor.len(), data.len() as u64);
            assert!(!encryptor.is_empty());
            unwrap!(encryptor.close().await)
        };

        match data_map {
            DataMap::Chunks(ref chunks) => assert_eq!(chunks.len(), 3),
            _ => panic!("Wrong DataMap type returned."),
        }

        let self_encryptor = unwrap!(SelfEncryptor::new(storage, data_map));
        let fetched = unwrap!(self_encryptor.read(0, data.len() as u64).await);
        assert_eq!(Blob(&fetched), Blob(data));
    }

    // Splits `data` into several pieces, then for each piece:
    //  * constructs a new encryptor from existing chunk details (except for the first piece)
    //  * writes the piece
    //  * closes and reads back the full data via a `SelfEncryptor`.
    async fn multiple_writes_then_close<T: Rng>(rng: &mut T, data: &[u8]) {
        let mut storage = SimpleStorage::new();
        let mut existing_data = vec![];
        let data_pieces = utils::make_random_pieces(rng, data, MIN as usize);
        let mut current_chunks = vec![];
        for data in data_pieces {
            let data_map = {
                let mut encryptor = if current_chunks.is_empty() {
                    unwrap!(SmallEncryptor::new(storage, vec![])
                        .await
                        .map(MediumEncryptor::from))
                } else {
                    unwrap!(MediumEncryptor::new(storage, current_chunks).await)
                };
                encryptor = unwrap!(encryptor.write(data).await);
                existing_data.extend_from_slice(data);
                assert_eq!(encryptor.len(), existing_data.len() as u64);

                let (data_map, storage2) = unwrap!(encryptor.close().await);
                storage = storage2;
                data_map
            };

            match data_map {
                DataMap::Chunks(ref chunks) => {
                    assert_eq!(chunks.len(), 3);
                    current_chunks = chunks.clone()
                }
                _ => panic!("Wrong DataMap type returned."),
            }

            let self_encryptor = unwrap!(SelfEncryptor::new(storage, data_map));
            assert_eq!(self_encryptor.len(), existing_data.len() as u64);
            let fetched = unwrap!(self_encryptor.read(0, existing_data.len() as u64).await);
            assert_eq!(fetched, existing_data);
            storage = self_encryptor.into_storage();
        }
        assert_eq!(Blob(&existing_data[..]), Blob(data));
    }

    #[tokio::test]
    async fn all_unit() {
        let mut rng = new_test_rng();
        let data = random_bytes(&mut rng, MAX as usize);

        basic_write_and_close(&data[..MIN as usize]).await;
        basic_write_and_close(&data[..MAX_CHUNK_SIZE as usize]).await;
        basic_write_and_close(&data[..(MAX_CHUNK_SIZE as usize * 2)]).await;
        basic_write_and_close(&data).await;

        multiple_writes_then_close(&mut rng, &data[..(MIN as usize * 2)]).await;
        multiple_writes_then_close(&mut rng, &data[..MAX_CHUNK_SIZE as usize]).await;
        multiple_writes_then_close(&mut rng, &data[..(MAX_CHUNK_SIZE as usize * 2)]).await;
        multiple_writes_then_close(&mut rng, &data).await;
    }
}
